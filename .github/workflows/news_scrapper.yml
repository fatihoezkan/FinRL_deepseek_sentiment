name: Daily Finance Scraper

# on:
#   schedule:
#     - cron: '0 */13 * * *'  # every 13 hours
#   workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      working-directory: ./scrape
      run: |
        pip install -r requirements.txt

    - name: Run the scraper
      working-directory: ./scrape
      env:
        REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
        REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
        REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        REDDIT_USERNAME: ${{ secrets.REDDIT_USERNAME }}
        REDDIT_PASSWORD: ${{ secrets.REDDIT_PASSWORD }}
        NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
      run: python scrape_script.py

    - name: Commit and push CSV + JSON
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git pull origin main
        cd scrape
        git add news.csv seen_hashes.json
        git commit -m "Automated daily scrape: $(date)"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
